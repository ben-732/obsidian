Sorting an array of integers

Firstly, make an observation, what is the first element in the sorted array. 

### Selection Algorithm
#implementable
Swapping the *next smallest* integer with the integer in the position it needs to go to.

```python
for i = 1, ..., n -1:
	min_index = i
	for j = i + 1, ..., n: 
		if(A[j] < a[min_ind]):
			min_ind = j
	swap(A[i], A[min_ind])
```

$$\Large O(n^2)$$

### Insertion Algorithm
#implementable
Progressively build a sorted array inserting each new number in the correct place

```python
for i = 2, ..., n-1:
	
	current_card = A[i]
	insert_after = i-1
	
	while (insert_after >= 1 && ( A[insert_after] > current_card ):
		
		A[insert_after+1] = A[insert_after]
		insert after--
		
	A[insert_ after+1] = current card
```

$$\Large O(n^2)$$

This has the same time complexity as selection sort algorithm, however if the array is *almost* sorted, then it will be significantly quicker. 

>[!extra]- Not examinable? - More time complexity
>A pair of indicies `i < j `that form an inversion in A, if `A[j] > A[i]`
>The time complexity here is $\Theta(n + \text{number of inversions})$
>



### Merge Sort
#implementable
==Take some notes maybe==
#todo
Divide and conquer method. 

$O(n\log n)$ running time ?


### Quicksort
#implementable
Split the array around a "centre" point called a pivot

Go through the array putting elements larger than the pivot in one array and elements smaller than the pivot in another. 

Sort these two arrays then recombine them. 

#### Implementation
```java
void quicksort(int A[], int start, int end) {
	if(start >= endd) return;
	
	int q = partition(A, start, end);
	quicksort(A, start, q-1);
	quicksort(A, q+1, end);
}
```


##### Finding the pivot:
`q = partition(A, start, end)`
Takes an array `A`. Uses the last element as the pivot then reshuffles the array around that point

>[!example]-
![[Pasted image 20230731171237.png]]
![[Pasted image 20230731171242.png]]

This can be achieved by iterating across the array from the pivot. If the number is greater than the pivot, move it to the right of the pivot (by moving it to the left of the pivot and then swapping with the pivot). If its less than the pivot, just move the pointer and continue. 

>[!example]-
>![[Pasted image 20230731171932.png]]

```java
int partition(int A[], int start, int end) {
	int q = end;
	for (int i = end - 1; i >= start; i --) { // Iterate across array from end
		if(A[q] < A[i]) {
			if(i != q-1) swap(A[i], A[q-1]); // If pointer isn't directly to the left of pivot
			swap(A[q-1], A[q]); // Swap pivot with element to the left of it
			q--;
		}
	}
	return q
}
```

This **is not** a standard textbook implementation. Worst case ~$2L$ swaps where $L$ is the length of the subarray. There exists implementations with at most $L/2$ swaps. *Hoare's* and *Lomuto's* partitioning.



#### Time Complexity
Worst Case: $\Theta( n^2)$

**Heuristic** solution: Choose a pivot
- Uniformly at random
- Median of the first, middle and the last element

True $\Theta( n \log n)$ solution:
There exists an $O(n)$ algorithm which finds the true median of a given array. 


#### Random Pivot Analysis
Average Case: $O(n\log n)$

Two types of average:
- Random input
- Any given input, randomness in computation

>[!example]- Example - Ideal Situation
![[Pasted image 20230731173300.png]]

The probability that we choose a perfect pivot at random is roughly ~$\frac{2}{n}$

We don't need the split to be ideal, we can get by with sizes $\frac{3n}{4}$ and $\frac{n}{4}$

No matter what, the work done by any level in the tree is at most $C\, n$. This is because the majority of the steps come from the partition function which is only called on disjoint subarrays.

This means that we only need to worry about the number of levels  

>[!example]- Example - Less ideal situation
>![[Pasted image 20230731174048.png]]

This means that there is roughly a $\frac{1}{2}$ chance of choosing a good pivot at random.  This is because the pivot has to be between $\frac{n}{4} < \frac{3n}{4}$

This is still assuming that we have a *good* split however. 

### Heapsort
This is a true in place $O(n\log n)$ sorting method.

#### Selection sorting using AVL Trees
[[Binary Trees?#AVL Trees |AVL Trees]] support `add/remove/findMax` all in $O(\log n)$

AVL selection sort: 
- Add all elements of the array A (of length n) into an AVL tree
- Repeat until $n=0$
	- `A[n] = avltree.findMax()`
	- `avlTree.remvoe(A[n])`
	- `n = n-1`

This has a running time of $O(n\log n)$, we are not using the full power of AVL trees here, just `add/findMax/removeMax`

`findMax` and `removeMax` are special cases of find and remove methods

#### Heapsort
This is an $n\log n$ in place sorting algorithm
Has two parts:
1. Organise the data in the form of a tree. 
	- This doesn't look very helpful
	- Re-shuffle the array so no element is larger than its parent. 
2. Map and fix the tree
	- Remove the max element and fix the tree
	- Swap the largest element to position $n$ then fix the new element in position 1

Because of the way the array as assigned, the index of a parent is always $\frac{{i-1}}{2}$ and the index of a child is always $2i + 1$ and $2i + 2$


##### Heapifying
Organising the data into a tree looks like this. 
![[Pasted image 20230823180736.png | center | 300]]

Next go through and reshuffle the array such that no node is larger than it's parent. Known as Heapifying

- Look at first and second element, if second element is smaller than first element, swap them. 
- Look at first and third element. If first is smaller, swap. 
- Look at index 3 and 1, if 3 is bigger than 1, swap. Then look at 1 and 0
- continue to the end of array comparing with parents. 

##### Sorting

At this stage the largest element is guaranteed to be leftmost (index 0).
This means to sort the array you can just swap the first element with the last element (n-1), then fix the tree to get it back to the heap pattern. 
Next the second largest element will be in index 0 so you can swap that with n-2 and fix the tree again. 

##### Fixing
Start from the parent node, swapping each node with the largest of its two children until you reach a node where both are smaller or there are no children. 

Children can be found at index's $2i + 1$ and $2i+2$
