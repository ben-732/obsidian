

Given an unsorted array, find the `k`-th smallest element in an array given $1 \leq k \leq n$

##### Boring way
You can just sort the array and return the `k`-th element. 

$O(n\log n)$

This is somewhat slow because of having to sort the array first. ]

Quick select has an average speed of $O(n)$

### Quick select
#implementable
`k-th smallest element` = the element at `k-1` if we were to sort the array

`k-1` here refers to an index

>[[3. Binary Search | Binary Search]] meets [[4. Sorting#Quicksort |quicksort]]
>

Standard binary search wouldn't work here as the array has no structured (has no order)
#### Step 1:
Choose a pivot and do first level sort like you would for quick sort
![[Pasted image 20230802172321.png]]

From here there are three cases:
1. `q`  = `k-1`. If this is the case then we don't care about either of the two unsorted sections because we know that the this element is going to be in the right location.
2. `k-1` < `q`: The green part (> `q`) is not going to effect the answer we are after no matter how it is sorted so we can ignore it 
3. `k-1` > `q`:  same as above but for the other section

We can do this repetitively in binary search style to narrow down the range of elements without having to sort the entire list.

```java
int quickSelect(int A[], int k) {
	int left = 0; int right = A.length - 1;

	while(left < right) {
		int q = partition(A, left, right);

		if(q == k - 1) return A[q];

		if((k-1) < q) right = q - 1;
		else if((k-1) > q) left = q + 1;
		
	}
	return A[left]
}
```

#### [[2. Time Complexity | Time Complexity]]
##### Worst case
In the worst scenario this algorithm (like quick sort) has a complexity of $O(n^2)$.

E.g. looking for `k = 1` in an already sorted array. The algorithm will keep choosing the last element as the pivot. 

##### Ideal Case
In an ideal scenario, the length of `[left, q-1]` is roughly equal to `[q+1, right]`. This would lead to the size of the partition to half with every iteration. 

Here the total number of operations is equal to:


$$
C_{n} + \frac{C_{n}}{2} + \frac{C_{n}}{4} + \dots \frac{C_{n}}{2^k} < C_{n} \cdot  2n
$$
The problem with this however, is that the probability of hitting a pivot which gives an ideal split is $\approx \frac{2}{n}$
Meaning that there is a very small chance that we are going to manage to hit it. 

This is the exact situation we had with quicksort. 

So we solve the same way we did with quicksort. We call the split good if the length of the subarrays are somewhere between $\frac{n}{4}$ and $\frac{3n}{4}$

##### Average case
On average this algorithm will preform with a time complexity of $O(n)$

There should be enough "good" and "bad" splits so that the total number of operations averages out to $O(n)$

There is a massive probability that every 10th split is good. And if every 10th split is good, then the complexity is still $O(n)$ as the good split makes up for the bad ones. 

> This doesn't quite prove that the average-case complexity is $O(n)$ but it provides good intuition.

There is an algorithm that gets the median with O(n) complexity but were not doing that here. 

That would mean that you do not have to choose random elements pivots, guaranteeing good splits. 
