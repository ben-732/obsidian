- Running time and time complexity
- Big O Notation
- Problem solving

### Running time
Many factors can contribute to the overall running time of a program

- Inputs
	- We always will assume that our program reads some data then outputs a result
	- The structure of the inputs also matter
- Number of CPU instructions
	- This depends on the programming language, compiler, OS, and more
- Memory
	- Accessing external memory is expensive
	- If you process a large amount of data, you are going to have to use external memory
	- Caching data helps reduce the number of external calls you need
- Network communication
	- RPC Calls are expensive

> In this course we are only going to focus on **inputs** and **CPU instructions**

#### Comparing `C++` and `Python`

Problem: Sort an array into increasing order

```
Input: [5, 4, 8, 13, -5, 2, 4, 2]
Output: [-5, 2, 2, 4, 4, 5, 8, 13]
```

The code to preform this action in C++ and Python:
![[C++ vs python code.png]]

The python code is shorter, but slower

Anti sorted list - furthest away from sorted possible.

Measuring the execution time with the `time` command:
![[Timing programs.png]]


##### Graphing runtimes
![[C++ vs Python preformance.png]]

> The scale is different but the curves look similar. 

The scale of these two graphs are different but the shape is overall the same. In both cases, when we increase the input size a little bit, the running time also increases a little bit. 

#### Definitions

The shape of the curve is inherent to the algorithm. Not the programming language or the computer. 

>[!info] Informal Definition
>The shape of the curve as a function of the input size (in the worst case senario), is what we call the Time Complexity of an algorithm. 


#### Finding the time complexity
![[Pasted image 20230719132431.png | centre | 200]]

Assume each operation in the algorithm
- The inner loop runs between `i+1` and `n` so it runs `n-i` times
- This means that the number of times the inner bit of the loops runs is:
	- $(n-1) + (n-2) + \dots +2 + 1 = \frac{(n-1)(n)}{2}$

Adding up all the components in the program gives something roughly equal to  $2n^2$

This in the end does not matter as it is effected by external factors like the programming language and the compiler. 

As a result we simplify this to say that the time complexity of this algorithm is 
$$\Large O(n^2)$$



### Big O Notation

Given [[Functions |function]] $f: N \to N$ and $g: N \to N$
$$\Large f = O(g)$$
if there exists constants $n_0$ and $C$ such that

$$\Large f(n) \leq C \,g(n)$$
for every $n \geq n_{0}$

>[!info]- Informal
>There is a constant you can multiply $g$ by so that $g(n)$ is always bigger than $f(n)$
>
>Eg: ![[Pasted image 20230719133905.png | centre| 300]]

>[!warning]- PROOF GOES HERER
>Something abt contradiction
>https://canvas.auckland.ac.nz/courses/89781/files/11279492?module_item_id=1905500

#### Sorting algorithm
We counted that at most $T(n) \leq 2n^2$ operations for input size $n$
This running time of $T(n)$ is this $O(n^2)$
- It is also technically $O(n^3)$, $O(n^4)$, $O(2^n)$
- It however is not $O(n)$ as $T(n) \geq \frac{n(n-1)}{2} \neq O(n)$

#### Big O effects?

|Notation | name|
|-|-|
|$O(1)$|Constant|
|$O(n\log n)$| Logarithmic|
|$O(n)$|linear|
|$O(n^2)$|Quadratic|
|$O(n³)$| Cubic|
|$O(2^n)$| Exponential|
| $O(n!)$ | Factorial|


#### A word of caution
- The goal of the big O notation is to hide the constants coming from specific languages/compilers and focus on the inherent rate of growth of the running time. 
- However, the algorithm itself might be such that it comes with large constants. 
- Example:
	- Suppose we have an algorithm which on some input, executes $5n² + 10n^2$ elementary operations (in our idealised programming language)
	- Yes, we still have $T(n) = O(n^2)$. However, $10^10n$ is much more relevant to the actual running time when when we implement it. That constant is only going to go up. 
	- These examples are very rare, and constants do play a role in comparing algorithms of the same time complexity. 

### Problem solving
New Zealand is currently hosting the FIFA Women's world cup. The organisers want to put a big welcome sign across buildings on the waterfront. 

![[Pasted image 20230719135325.png]]

There are no gaps between buildings and the width of each building is constant (10m). The goal is to come up with the maximum area that the sign can take up without overhanging the edge of any building. 

Inputs: `array of heights`
Output: `area of the largest banner`

#### Observations
Start by making some observations about the solution
1. The largest possible sign will always go from the beginning of one building to the end of another
2. If the largest sign must always go from the ground to the top of at least one building. 

#### Algorithm 1
Using these observations, go through each combination of `i` and `j` (being two buildings) and find the shortest building between them. The largest sign between i and j is the width times the height of the shortest building. 

Try this for every pair of buildings `i` and `j`

```python
largest_area = 0
for i = 1, ..., n:
	for j = i, ..., n:
		shortest = h[i]
		for k = i+1, ..., j:
			if shortest > h[k]
				shortest = h[k]
				
		area_ij = 10 * (j-i+1) * shortest
		if area_ij > largest_area:
			largest_area = area_ij
```

###### Time Complexity of this algorithm
$O(n^3)$

This is found by counting the number of triples $(i, j, k)$ such that $i < k \leq j$

There are (approximately) $\frac{n}{3}$ choices for i, j and k such that this criteria is fulfilled.

$$
\frac{n}{3} \times\frac{n}{3}\times\frac{n}{3} = \frac{n^3}{3^3}
$$


==There some funky maths to be put here  - lecture 3==

#### Algorithm 2
Store the shortest building as you go along. Rather than looping through all the buildings again to find the shortest when we can find it simply already. 

```python
largest_area = 0
for i = 1, ..., n:
	shortest = h[i]
	for j = i, ..., n:
	
		if shortest > h[j]
			shortest = h[j]
		
		area_ij = 10 * (j-i+1) * shortest
		if area_ij > largest_area:
			largest_area = area_ij
```

Time Complexity $O(n²)$


## Summary
$T = O(g)$ - $T$ is at most something like $g$
$T = \Omega(g)$ - $T$ is at least something like $g$
$T = \Theta(g)$ - $T$ is at something like $g$

